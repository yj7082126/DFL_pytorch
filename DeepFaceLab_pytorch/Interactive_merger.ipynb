{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from core.models import DF, LIAE\n",
    "from core.options import read_yaml\n",
    "from core.loglib import load_weights\n",
    "from DFLIMG import DFLIMG\n",
    "from facelib import FaceType, LandmarksProcessor\n",
    "from merger.MergeMasked import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"\"\n",
    "video = \"\"\n",
    "subfolder = \"\"\n",
    "model = \"\"\n",
    "outvideo = \"\"\n",
    "\n",
    "input_path = Path(f\"../video-input/{project}/{video}\")\n",
    "aligned_path = Path(f\"../video-input/{project}/{video}/aligned\")\n",
    "\n",
    "saved_models_path = Path(f\"../workspace/{project}/{model}_torch\")\n",
    "\n",
    "test_path = Path(f\"../video-output/{project}/{subfolder}/{outvideo}_raw\")\n",
    "test_mask_path = Path(f\"../video-output/{project}/{subfolder}/{outvideo}_raw_mask\")\n",
    "test_path.mkdir(parents=True, exist_ok=True)\n",
    "test_mask_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_path = Path(f\"../video-output/{project}/{subfolder}/{outvideo}2\")\n",
    "output_mask_path = Path(f\"../video-output/{project}/{subfolder}/{outvideo}2_mask\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "output_mask_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outputvideo_path = Path(f\"../video-output/{project}/{subfolder}/output_{outvideo}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder\n",
      "Loading inter_AB\n",
      "Loading inter_B\n",
      "Loading decoder\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "model_dict = read_yaml(saved_models_path.joinpath(\"model_opt.yaml\"))\n",
    "if model_dict.model_type.startswith('df'):\n",
    "    model = DF(model_dict.resolution, \n",
    "               model_dict.ae_dims, model_dict.e_dims, \n",
    "               model_dict.d_dims, model_dict.d_mask_dims, \n",
    "               likeness=model_dict.likeness, double_res=model_dict.double_res).to(device)\n",
    "else:\n",
    "    model = LIAE(model_dict.resolution, \n",
    "               model_dict.ae_dims, model_dict.e_dims, \n",
    "               model_dict.d_dims, model_dict.d_mask_dims, \n",
    "               likeness=model_dict.likeness, double_res=model_dict.double_res).to(device)\n",
    "    \n",
    "model, log_history = load_weights(saved_models_path, model, finetune_start=False)\n",
    "\n",
    "predictor_input_shape = (model_dict.resolution, model_dict.resolution, 3)\n",
    "face_type = model_dict.face_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = {}\n",
    "input_path_image_paths = sorted(list(input_path.glob(\"*.jpg\")))\n",
    "align_path_image_paths = sorted(list(aligned_path.glob(\"*.jpg\")))\n",
    "\n",
    "for filepath in tqdm(align_path_image_paths):\n",
    "    filepath = Path(filepath)\n",
    "    dflimg = DFLIMG.load(filepath)\n",
    "                    \n",
    "    if dflimg is None or not dflimg.has_data():\n",
    "        print(f\"{filepath.name} is not a dfl image file\")\n",
    "        continue\n",
    "    else:\n",
    "        source_filename = dflimg.get_source_filename()\n",
    "        if source_filename is None:\n",
    "            continue\n",
    "        else:\n",
    "            source_filename_stem = Path(source_filename).stem\n",
    "            if source_filename_stem not in alignments.keys():\n",
    "                alignments[source_filename_stem] = [dflimg.get_source_landmarks()]\n",
    "\n",
    "frames = [ \n",
    "    {\n",
    "        \"filepath\" : Path(p),\n",
    "        \"landmarks_list\" : alignments.get(Path(p).stem, None),\n",
    "    }\n",
    "    for p in input_path_image_paths \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_info = frames[3150]\n",
    "\n",
    "img_bgr_uint8 = cv2.imread(str(frame_info[\"filepath\"]))\n",
    "img_bgr = img_bgr_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "img_face_landmarks = frame_info[\"landmarks_list\"][0]\n",
    "img_size = img_bgr.shape[1], img_bgr.shape[0]\n",
    "img_face_mask_a = LandmarksProcessor.get_image_hull_mask(img_bgr.shape, img_face_landmarks)\n",
    "\n",
    "size_dict, mat_dict = get_size_and_mat(predictor_input_shape, img_face_landmarks, \n",
    "            1.0, face_type, use_sr = False)\n",
    "\n",
    "dst_face_bgr      = warp_and_clip(img_bgr,         mat_dict[\"face\"], size_dict[\"output\"])\n",
    "dst_face_mask_a_0 = warp_and_clip(img_face_mask_a, mat_dict[\"face\"], size_dict[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    warped_dst = torch.from_numpy(dst_face_bgr).permute(2,0,1).unsqueeze(0).to(device)\n",
    "    prd_face_bgr, prd_face_mask_a_0, prd_face_dst_mask_a_0 = model.single_forward(warped_dst)\n",
    "    prd_face_bgr = prd_face_bgr[0].permute(1,2,0).detach().cpu().numpy()\n",
    "    prd_face_mask_a_0 = prd_face_mask_a_0[0].permute(1,2,0).detach().cpu().numpy()\n",
    "    prd_face_dst_mask_a_0 = prd_face_dst_mask_a_0[0].permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "prd_face_bgr = cv2.resize(prd_face_bgr, (size_dict[\"input\"], size_dict[\"input\"]), cv2.INTER_CUBIC)\n",
    "\n",
    "prd_full = warp_and_clip(prd_face_bgr, mat_dict[\"face_output\"], img_size, inverse=True)\n",
    "dst_full = warp_and_clip(dst_face_bgr, mat_dict[\"face_output\"], img_size, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_face_mask_a_0 = get_normfacemask(dst_face_mask_a_0, prd_face_mask_a_0, prd_face_dst_mask_a_0, \n",
    "                         mask_mode=3, output_size=size_dict[\"output\"])[:,:,0]\n",
    "if wrk_face_mask_a_0.shape[0] != size_dict[\"mask\"]:\n",
    "    wrk_face_mask_a_0 = cv2.resize (wrk_face_mask_a_0, \n",
    "        (size_dict[\"mask\"], size_dict[\"mask\"]), \n",
    "        interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "wrk_face_mask_b_0 = get_erodeblurmask(wrk_face_mask_a_0, ero=30, blur=80, input_size=size_dict[\"input\"])\n",
    "wrk_face_mask_b_0 = np.clip(wrk_face_mask_b_0, 0.0, 1.0)\n",
    "\n",
    "img_face_mask_a = warp_and_clip(wrk_face_mask_b_0, mat_dict[\"face_mask\"], \n",
    "            img_size, use_clip=True, inverse=True, remove_noise=True)\n",
    "\n",
    "if wrk_face_mask_b_0.shape[0] != size_dict[\"output\"]:\n",
    "    wrk_face_mask_b_0 = cv2.resize (wrk_face_mask_b_0, \n",
    "        (size_dict[\"output\"],size_dict[\"output\"]), \n",
    "        interpolation=cv2.INTER_CUBIC)\n",
    "                \n",
    "wrk_face_mask_area = wrk_face_mask_b_0[...,None].copy()\n",
    "wrk_face_mask_area[wrk_face_mask_area>0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(18,6))\n",
    "ax[0].imshow(dst_face_bgr[:,:,::-1])\n",
    "ax[1].imshow(prd_face_bgr[:,:,::-1])\n",
    "ax[2].imshow(wrk_face_mask_a_0, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_face_bgr_c = prd_face_bgr.copy()\n",
    "prd_face_bgr_c = get_colortransfer(prd_face_bgr_c, dst_face_bgr, wrk_face_mask_area, \n",
    "    color_transfer_mode=2)\n",
    "\n",
    "out_img = warp_and_clip(prd_face_bgr_c, mat_dict[\"face_output\"], img_size, inverse=True)\n",
    "\n",
    "out_img = img_bgr*(1-img_face_mask_a) + out_img*img_face_mask_a\n",
    "\n",
    "out_face = warp_and_clip(out_img, mat_dict[\"face\"], size_dict[\"output\"], use_clip=False)\n",
    "\n",
    "plt.imshow(out_face[:,:,::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}